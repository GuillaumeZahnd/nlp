{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be55fc4-3a68-4413-92e8-f7959a6212ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "d = dirname(dirname(abspath(\"__file__\")))\n",
    "sys.path.insert(0, str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc64c6e-6ab2-4400-98f8-ee7b82aac248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7768a065-896d-443e-af81-60d62ea4465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import random\n",
    "main_rng = random.PRNGKey(421)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff5abe1-7c24-49d8-91bf-4b4286a398f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344c5606-6d87-41a6-b576-809719ca9ad9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (jax_transformer_display_helpers.py, line 3)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/media/guillaume/DATA/NERD/GitHub/nlp/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 3\u001b[0;36m\n\u001b[0;31m    from jax_transformer_display_helpers import display_scaled_dot_product\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/media/guillaume/DATA/NERD/GitHub/nlp/jax_transformer_mlm/jax_transformer_display_helpers.py:3\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ../utils_display import nice_colorbar\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from utils_display import nice_colorbar\n",
    "from utils_display import pc\n",
    "from jax_transformer_display_helpers import display_scaled_dot_product\n",
    "from jax_transformer_display_helpers import display_positional_encoding\n",
    "from jax_transformer_display_helpers import display_positional_encoding_profiles\n",
    "from jax_transformer_display_helpers import display_lr_scheduler\n",
    "\n",
    "from jax_transformer import MultiheadAttention\n",
    "from jax_transformer import EncoderBlock\n",
    "from jax_transformer import TransformerEncoder\n",
    "from jax_transformer import scaled_dot_product\n",
    "from jax_transformer import expand_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695f1de-917f-4dab-a688-72063d15fee0",
   "metadata": {},
   "source": [
    "# Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b2de8-26fc-4721-a636-ba9b01ac77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "embedding_dimensionality = 3\n",
    "\n",
    "_, rand1 = random.split(main_rng)\n",
    "\n",
    "qkv = random.normal(rand1, (3, sequence_length, embedding_dimensionality))\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "mask = jnp.zeros((sequence_length, sequence_length))\n",
    "mask = mask.at[3,:].set(1)\n",
    "\n",
    "weighted_sum_of_values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
    "                 \n",
    "display_scaled_dot_product(q, k, v, mask, weighted_sum_of_values, attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322d373-cdfd-4b85-83f8-fc5e72f5c6fc",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2064080-080f-4970-aa68-e61b4c0c5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test MultiheadAttention implementation\n",
    "\n",
    "batch_size = 2\n",
    "sequence_length = 13\n",
    "embedding_dimensionality = 32\n",
    "number_of_heads = 4\n",
    "\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "\n",
    "x = random.normal(x_rng, (batch_size, sequence_length, embedding_dimensionality))\n",
    "\n",
    "mha = MultiheadAttention(embedding_dimensionality=embedding_dimensionality, number_of_heads=number_of_heads)\n",
    "\n",
    "main_rng, init_rng = random.split(main_rng)\n",
    "\n",
    "params = mha.init(init_rng, x)['params']\n",
    "w_o, attention_weights = mha.apply({'params': params}, x)\n",
    "\n",
    "print('Out', w_o.shape, 'Attention', attention_weights.shape)\n",
    "\n",
    "del w_o, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6782902-5177-4bf3-9be9-334335c0ba61",
   "metadata": {},
   "source": [
    "# Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dc391-4285-421d-866a-b7ca98197d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test EncoderBlock implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create encoder block\n",
    "encblock = EncoderBlock(input_dimensionality=128, number_of_heads=4, feedforward_dimensionality=512, dropout_probability=0.1)\n",
    "# Initialize parameters of encoder block with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = encblock.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "# Apply encoder block with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "out = encblock.apply({'params': params}, x, train=True, rngs={'dropout': dropout_apply_rng})\n",
    "print('Out', out.shape)\n",
    "\n",
    "del encblock, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7772f-a896-4cd2-86e6-99d1f318a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test TransformerEncoder implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create Transformer encoder\n",
    "transenc = TransformerEncoder(\n",
    "    number_of_layers=5,\n",
    "    input_dimensionality=128,\n",
    "    number_of_heads=4,\n",
    "    feedforward_dimensionality=256,\n",
    "    dropout_probability=0.15)\n",
    "\n",
    "# Initialize parameters of transformer with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = transenc.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "# Apply transformer with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "# Instead of passing params and rngs every time to a function call, we can bind them to the module\n",
    "binded_mod = transenc.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "out = binded_mod(x, train=True)\n",
    "print('Out', out.shape)\n",
    "attn_maps = binded_mod.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attn_maps), attn_maps[0].shape)\n",
    "\n",
    "del transenc, binded_mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5138c63-d1b4-4c78-b207-cee6d551265a",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ded427-be50-47a1-870f-20d5bce89038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    hidden_dimensionality : int\n",
    "    maximum_sequence_length : int = 5000\n",
    "\n",
    "    def setup(self):\n",
    "        positional_encoding = np.zeros((self.maximum_sequence_length, self.hidden_dimensionality))\n",
    "        position = np.arange(0, self.maximum_sequence_length, dtype=np.float32)[:,None]\n",
    "        denominator = np.exp(np.arange(0, self.hidden_dimensionality, 2) * (-math.log(10000.0) / self.hidden_dimensionality))\n",
    "        positional_encoding[:, 0::2] = np.sin(position * denominator)\n",
    "        positional_encoding[:, 1::2] = np.cos(position * denominator)\n",
    "        positional_encoding = positional_encoding[None]\n",
    "        self.positional_encoding = jax.device_put(positional_encoding)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.positional_encoding[:, :x.shape[1]]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a9b7a-fe79-47c2-a019-0a14eef1c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoding block, bind to access positional encoding (module has no parameters)\n",
    "encod_block = PositionalEncoding(hidden_dimensionality=48, maximum_sequence_length=96).bind({})\n",
    "# Obtain positional encodings as numpy array\n",
    "positional_encoding = jax.device_get(encod_block.positional_encoding.squeeze().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd23576f-0b72-40c6-9269-6188fc724205",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_positional_encoding(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c847c3d-99ce-4312-b1de-07a47b6d56f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_positional_encoding_profiles(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dcfac-a44f-495d-9d80-c806e1f89f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e138d08-4e4d-43f7-827a-ea078afc3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_warmup_schedule(base_lr: float, warmup: int, max_iters: int):\n",
    "    assert warmup > 0 and max_iters > 0\n",
    "    # Create function to return lr based on iteration count\n",
    "    def get_lr(train_iter):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * train_iter / max_iters))\n",
    "        if train_iter <= warmup:\n",
    "            lr_factor *= train_iter * 1.0 / warmup\n",
    "        return lr_factor * base_lr\n",
    "    return get_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61738839-ab4f-4934-849a-2c4fc21414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = cosine_warmup_schedule(base_lr=1.0, warmup=100, max_iters=2000)\n",
    "\n",
    "display_lr_scheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1c965-287b-4717-8134-ac9f9fb5a28a",
   "metadata": {},
   "source": [
    "# Full transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd840ee-b894-4100-915d-8cf7c739ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(nn.Module):\n",
    "    model_dim : int                   # Hidden dimensionality to use inside the Transformer\n",
    "    num_classes : int                 # Number of classes to predict per sequence element\n",
    "    num_heads : int                   # Number of heads to use in the Multi-Head Attention blocks\n",
    "    num_layers : int                  # Number of encoder blocks to use\n",
    "    dropout_prob : float = 0.0        # Dropout to apply inside the model\n",
    "    input_dropout_prob : float = 0.0  # Dropout to apply on the input features\n",
    "\n",
    "    def setup(self):\n",
    "        \n",
    "        self.input_dropout = nn.Dropout(self.input_dropout_prob)\n",
    "        self.input_layer = nn.Dense(self.model_dim)        \n",
    "        self.positional_encoding = PositionalEncoding(self.model_dim)\n",
    "        feedforward_dimensionality = 4 * self.model_dim\n",
    "        \n",
    "        self.transformer = TransformerEncoder(\n",
    "            number_of_layers=self.num_layers,\n",
    "            input_dimensionality=self.model_dim,\n",
    "            number_of_heads=self.num_heads,\n",
    "            feedforward_dimensionality=feedforward_dimensionality,\n",
    "            dropout_probability=self.dropout_prob)\n",
    "\n",
    "        # Output classifier to convert each token vector into a probability distribution over the token vocabulary\n",
    "        # To perforem next token prediction\n",
    "        self.output_net = [\n",
    "            nn.Dense(self.model_dim),\n",
    "            nn.LayerNorm(),\n",
    "            nn.relu,\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.Dense(self.num_classes)]\n",
    "\n",
    "    def __call__(self, x, mask=None, add_positional_encoding=True, train=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "            train - If True, dropout is stochastic\n",
    "        \"\"\"\n",
    "        x = self.input_dropout(x, deterministic=not train)\n",
    "        x = self.input_layer(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask, train=train)\n",
    "\n",
    "\n",
    "        for l in self.output_net:\n",
    "            x = l(x) if not isinstance(l, nn.Dropout) else l(x, deterministic=not train)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True, train=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        x = self.input_dropout(x, deterministic=not train)\n",
    "        x = self.input_layer(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask, train=train)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0d7a6-b150-4db9-9813-f4a98f5ab480",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_rng, x_rng = random.split(main_rng)\n",
    "\n",
    "x = random.normal(x_rng, (3, 16, 1))\n",
    "\n",
    "transpre = TransformerPredictor(num_layers=5,\n",
    "                                model_dim=128,\n",
    "                                num_classes=10,\n",
    "                                num_heads=4,\n",
    "                                dropout_prob=0.15,\n",
    "                                input_dropout_prob=0.05)\n",
    "\n",
    "# Initialize parameters of transformer predictor with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = transpre.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "\n",
    "print('[initialization finished]')\n",
    "\n",
    "# Apply transformer predictor with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "\n",
    "# Instead of passing params and rngs every time to a function call, we can bind them to the module\n",
    "model = transpre.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "\n",
    "\n",
    "out = model(x, mask=None, add_positional_encoding=True, train=True)\n",
    "print('Out', out.shape)\n",
    "\n",
    "\n",
    "attn_maps = model.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attn_maps), attn_maps[0].shape)\n",
    "\n",
    "del transpre, model, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dde69-4b1b-4a52-ae60-499616b5ba50",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818df5bf-a5ba-4f73-b4c3-328a7aae8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, model_name, exmp_batch, max_iters, lr=1e-3, warmup=100, seed=42, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model. Used for saving and checkpointing\n",
    "            exmp_batch - Example batch to the model for initialization\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            seed - Seed to use for model init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.max_iters = max_iters\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = TransformerPredictor(**model_kwargs)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, self.model_name)\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_batch)\n",
    "\n",
    "    def batch_to_input(self, exmp_batch):\n",
    "        # Map batch to input data to the model\n",
    "        # To be implemented in a task specific sub-class\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        # Return a function that calculates the loss for a batch\n",
    "        # To be implemented in a task specific sub-class\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_functions(self):\n",
    "        # Create jitted train and eval functions\n",
    "        calculate_loss = self.get_loss_function()\n",
    "\n",
    "        # Training function\n",
    "        def train_step(state, rng, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, rng, batch, train=True)\n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, acc, rng = ret[0], *ret[1]\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "        self.train_step = jax.jit(train_step)\n",
    "\n",
    "        # Evaluation function\n",
    "        def eval_step(state, rng, batch):\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return acc, rng\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def init_model(self, exmp_batch):\n",
    "        # Initialize model\n",
    "        self.rng = jax.random.PRNGKey(self.seed)\n",
    "        self.rng, init_rng, dropout_init_rng = jax.random.split(self.rng, 3)\n",
    "        exmp_input = self.batch_to_input(exmp_batch)\n",
    "        params = self.model.init({'params': init_rng, 'dropout': dropout_init_rng}, exmp_input, train=True)['params']\n",
    "        # Initialize learning rate schedule and optimizer\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=self.lr,\n",
    "            warmup_steps=self.warmup,\n",
    "            decay_steps=self.max_iters,\n",
    "            end_value=0.0\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adam(lr_schedule)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=optimizer)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=500):\n",
    "        # Train model for defined number of epochs\n",
    "        best_acc = 0.0\n",
    "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
    "            self.train_epoch(train_loader, epoch=epoch_idx)\n",
    "            if epoch_idx % 5 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar('val/accuracy', eval_acc, global_step=epoch_idx)\n",
    "                if eval_acc >= best_acc:\n",
    "                    best_acc = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        accs, losses = [], []\n",
    "        for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "            self.state, self.rng, loss, accuracy = self.train_step(self.state, self.rng, batch)\n",
    "            losses.append(loss)\n",
    "            accs.append(accuracy)\n",
    "        avg_loss = np.stack(jax.device_get(losses)).mean()\n",
    "        avg_acc = np.stack(jax.device_get(accs)).mean()\n",
    "        self.logger.add_scalar('train/loss', avg_loss, global_step=epoch)\n",
    "        self.logger.add_scalar('train/accuracy', avg_acc, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all data points of a data loader and return avg accuracy\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            acc, self.rng = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir, target=self.state.params, step=step)\n",
    "\n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for the pretrained model\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=self.state.params)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'), target=self.state.params)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.state.tx)\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this Transformer\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e564d8-48df-42df-86b3-b28499bd9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        with open(os.path.join(\"local_datasets\", \"wikipedia_man_o_war.pkl\"), \"rb\") as fid:                                       \n",
    "            self.dico_word2index, self.dico_index2word, self.dataset = pickle.load(fid) \n",
    "        \n",
    "        self.num_categories = len(self.dico_word2index)\n",
    "        self.maximum_sequence_length = len(self.dataset[0][\"mask\"])\n",
    "        self.size = len(self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_indices = self.dataset[index][\"input_indices\"]\n",
    "        mask = self.dataset[index][\"mask\"]\n",
    "        masked_indices = self.dataset[index][\"masked_indices\"]\n",
    "        labels = self.dataset[index][\"labels\"]\n",
    "        return input_indices, mask, masked_indices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a7af7-6a32-410f-9490-d7a1405bd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MLMDataset()\n",
    "\n",
    "print(dataset.num_categories)\n",
    "print(dataset.maximum_sequence_length)\n",
    "print(dataset.size)\n",
    "\n",
    "data_loader = data.DataLoader(dataset)\n",
    "\n",
    "index = 7\n",
    "input_indices, mask, masked_indices, labels = data_loader.dataset[index]\n",
    "\n",
    "pc(\"Input indices\", input_indices)\n",
    "pc(\"Mask\", mask)\n",
    "pc(\"Masked indices\", masked_indices)\n",
    "pc(\"Labels\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3adec-4bc7-4777-8928-00d466a4a3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
