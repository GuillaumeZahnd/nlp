{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85c8523-9996-4840-ac8f-023099b1f05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 17:33:14.857399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750865594.870168  125041 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750865594.874146  125041 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750865594.883816  125041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750865594.883834  125041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750865594.883836  125041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750865594.883837  125041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-25 17:33:14.888522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3b29d0-80f4-435b-9b8c-aadb4c9f4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_display import pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf7d61c-12a0-4552-9ac6-b077c5a58593",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(421)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956368f2-ad29-43f5-9fde-65088a96a008",
   "metadata": {},
   "source": [
    "### Text, corpus, and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abd0aa-0336-44b8-813e-d69d403856b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text\n",
    "with open(\"datasets/animal_farm_george_orwell.txt\", \"r\") as fid:\n",
    "    text = fid.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0338b574-5807-472a-a608-69c59ffccba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNumber of sentences\u001b[0m: 1637\n"
     ]
    }
   ],
   "source": [
    "# Corpus\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "corpus = []\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.findall(r'\\b\\w+\\b', sentence) \n",
    "    corpus.append(sentence)\n",
    "\n",
    "number_of_sentences = len(corpus)\n",
    "pc(\"Number of sentences\", number_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc3e033-e07c-41c5-a047-55ec31971fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNumber of train sentences\u001b[0m: 1473\n",
      "\u001b[34mNumber of validation sentences\u001b[0m: 164\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "number_of_train_sentences = int(train_split * number_of_sentences)\n",
    "number_of_validation_sentences = number_of_sentences - number_of_train_sentences\n",
    "pc(\"Number of train sentences\", number_of_train_sentences)\n",
    "pc(\"Number of validation sentences\", number_of_validation_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91732f9f-e5b2-469a-82ce-5f23437c21ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "From now onwards Animal Farm would engage in trade with the neighbouring farms: not, of course, for any commercial purpose, but simply in order to obtain certain materials which were urgently necessary.\n",
      "['from', 'now', 'onwards', 'animal', 'farm', 'would', 'engage', 'in', 'trade', 'with', 'the', 'neighbouring', 'farms', 'not', 'of', 'course', 'for', 'any', 'commercial', 'purpose', 'but', 'simply', 'in', 'order', 'to', 'obtain', 'certain', 'materials', 'which', 'were', 'urgently', 'necessary']\n",
      "----------------------------------------------------------------\n",
      "1\n",
      "Finally there came a night when the gale was so violent that the farm buildings rocked on their foundations and several tiles were blown off the roof of the barn.\n",
      "['finally', 'there', 'came', 'a', 'night', 'when', 'the', 'gale', 'was', 'so', 'violent', 'that', 'the', 'farm', 'buildings', 'rocked', 'on', 'their', 'foundations', 'and', 'several', 'tiles', 'were', 'blown', 'off', 'the', 'roof', 'of', 'the', 'barn']\n",
      "----------------------------------------------------------------\n",
      "2\n",
      "The harvest is more important.\n",
      "['the', 'harvest', 'is', 'more', 'important']\n",
      "----------------------------------------------------------------\n",
      "3\n",
      "Clover was an old stout mare now, stiff in the joints and with a tendency to rheumy eyes.\n",
      "['clover', 'was', 'an', 'old', 'stout', 'mare', 'now', 'stiff', 'in', 'the', 'joints', 'and', 'with', 'a', 'tendency', 'to', 'rheumy', 'eyes']\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "number_of_example_sentences = 4\n",
    "for index in range(number_of_example_sentences):\n",
    "    print(index)\n",
    "    print(sentences[index])\n",
    "    print(corpus[index])\n",
    "    print(\"-\"*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2595233-94c0-440f-b461-df971e9bd609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mVocabulary size\u001b[0m: 3921\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocabulary = set([word for sentence in corpus for word in sentence])\n",
    "vocabulary_size = len(vocabulary)\n",
    "pc(\"Vocabulary size\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c14d54e-2c3c-4392-a98e-a0da830008bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency = {}\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        if word in words_frequency:\n",
    "            words_frequency[word] += 1\n",
    "        else:\n",
    "            words_frequency[word] = 1\n",
    "\n",
    "words_frequency = sorted(words_frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "#print(words_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9133d113-2f5f-4f0b-b119-7d5019f66ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: index for index, word in enumerate(vocabulary)}\n",
    "index2word = {index: word for index, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c060325-e9a7-4e1c-8345-392c191a13bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index\n",
      "[('cowman', 0), ('proclaimed', 1), ('legend', 2), ('dear', 3)]\n",
      "index2word\n",
      "[(0, 'cowman'), (1, 'proclaimed'), (2, 'legend'), (3, 'dear')]\n"
     ]
    }
   ],
   "source": [
    "number_of_example_entries = 4\n",
    "print(\"word2index\")\n",
    "print(list(word2index.items())[:number_of_example_entries])\n",
    "print(\"index2word\")\n",
    "print(list(index2word.items())[:number_of_example_entries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "780706e3-3bfd-4dd1-89b7-0702f4aff366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_and_label_pairs(sequences, context_window):    \n",
    "    context_and_target_pairs = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sequence in sequences:\n",
    "        for i in range(context_window, len(sequence) - context_window):\n",
    "            context = sequence[i - context_window : i] + sequence[i + 1 : i + context_window + 1]\n",
    "            target = sequence[i]\n",
    "            context_and_target_pairs.append((context, target))\n",
    "        \n",
    "            inputs.append([word2index[word] for word in context])\n",
    "            labels.append(word2index[target])\n",
    "\n",
    "    inputs = np.asarray(inputs)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    return context_and_target_pairs, inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eff32e3-2c38-41dd-9b04-ebfb9789a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNumber of training samples\u001b[0m: 21647\n",
      "\u001b[34mNumber of validation samples\u001b[0m: 2317\n"
     ]
    }
   ],
   "source": [
    "context_window = 2\n",
    "train_data, train_inputs, train_labels = generate_input_and_label_pairs(\n",
    "    sequences=corpus[:number_of_train_sentences], context_window=context_window)\n",
    "\n",
    "validation_data, validation_inputs, validation_labels = generate_input_and_label_pairs(\n",
    "    sequences=corpus[number_of_train_sentences:], context_window=context_window)\n",
    "\n",
    "pc(\"Number of training samples\", len(train_data))\n",
    "pc(\"Number of validation samples\", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea90295-7df0-484c-916e-b96b35b968a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1629 onwards              Input: [2841 1951 3874  248] (['from', 'now', 'animal', 'farm'])\n",
      "Label: 3874 animal               Input: [1951 1629  248 3884] (['now', 'onwards', 'farm', 'would'])\n",
      "Label: 248 farm                 Input: [1629 3874 3884 2170] (['onwards', 'animal', 'would', 'engage'])\n",
      "Label: 3884 would                Input: [3874  248 2170  913] (['animal', 'farm', 'engage', 'in'])\n"
     ]
    }
   ],
   "source": [
    "number_of_examples = 4\n",
    "for index in range(number_of_examples):\n",
    "    print(\"Label: {:<3} {:20} Input: {} ({})\".format(\n",
    "        train_labels[index], train_data[index][1], train_inputs[index], train_data[index][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1204a-9092-4629-beaa-08398c80092c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6017e44-418a-459b-8864-693f9452e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(vocabulary_size, activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c955d806-604a-4e5e-9cc9-673b061576f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750865597.005245  125041 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5693 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900ead6-4fa8-420d-b014-95f9550df877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750865597.534715  125079 service.cc:152] XLA service 0x764a04005cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750865597.534731  125079 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 2060 SUPER, Compute Capability 7.5\n",
      "2025-06-25 17:33:17.549162: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1750865597.596672  125079 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/677\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0606 - loss: 8.2487"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750865598.083762  125079 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0672 - loss: 7.8977 - val_accuracy: 0.0729 - val_loss: 6.8036\n",
      "Epoch 2/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0686 - loss: 6.5641 - val_accuracy: 0.0729 - val_loss: 6.6847\n",
      "Epoch 3/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0704 - loss: 6.3775 - val_accuracy: 0.0729 - val_loss: 6.6380\n",
      "Epoch 4/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0699 - loss: 6.2999 - val_accuracy: 0.0729 - val_loss: 6.6054\n",
      "Epoch 5/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0718 - loss: 6.2351 - val_accuracy: 0.0729 - val_loss: 6.5820\n",
      "Epoch 6/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0709 - loss: 6.1695 - val_accuracy: 0.0794 - val_loss: 6.5591\n",
      "Epoch 7/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0761 - loss: 6.1084 - val_accuracy: 0.0893 - val_loss: 6.5344\n",
      "Epoch 8/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0875 - loss: 6.0559 - val_accuracy: 0.0915 - val_loss: 6.5118\n",
      "Epoch 9/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0841 - loss: 6.0206 - val_accuracy: 0.0928 - val_loss: 6.4902\n",
      "Epoch 10/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0897 - loss: 5.9790 - val_accuracy: 0.0924 - val_loss: 6.4776\n",
      "Epoch 11/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0912 - loss: 5.8933 - val_accuracy: 0.0945 - val_loss: 6.4629\n",
      "Epoch 12/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0947 - loss: 5.8604 - val_accuracy: 0.1010 - val_loss: 6.4604\n",
      "Epoch 13/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0948 - loss: 5.8203 - val_accuracy: 0.1006 - val_loss: 6.4521\n",
      "Epoch 14/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1029 - loss: 5.7427 - val_accuracy: 0.1032 - val_loss: 6.4441\n",
      "Epoch 15/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1064 - loss: 5.6986 - val_accuracy: 0.1088 - val_loss: 6.4355\n",
      "Epoch 16/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1049 - loss: 5.6690 - val_accuracy: 0.1101 - val_loss: 6.4288\n",
      "Epoch 17/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1124 - loss: 5.5725 - val_accuracy: 0.1131 - val_loss: 6.4185\n",
      "Epoch 18/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1151 - loss: 5.5468 - val_accuracy: 0.1157 - val_loss: 6.4109\n",
      "Epoch 19/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1220 - loss: 5.4823 - val_accuracy: 0.1183 - val_loss: 6.4056\n",
      "Epoch 20/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1279 - loss: 5.4243 - val_accuracy: 0.1178 - val_loss: 6.4009\n",
      "Epoch 21/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1283 - loss: 5.3934 - val_accuracy: 0.1191 - val_loss: 6.3965\n",
      "Epoch 22/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1319 - loss: 5.3437 - val_accuracy: 0.1204 - val_loss: 6.3926\n",
      "Epoch 23/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1378 - loss: 5.2999 - val_accuracy: 0.1230 - val_loss: 6.3918\n",
      "Epoch 24/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1383 - loss: 5.2663 - val_accuracy: 0.1256 - val_loss: 6.3928\n",
      "Epoch 25/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.1398 - loss: 5.2552 - val_accuracy: 0.1260 - val_loss: 6.3936\n",
      "Epoch 26/50\n",
      "\u001b[1m677/677\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.1454 - loss: 5.1888 - val_accuracy: 0.1273 - val_loss: 6.3991\n",
      "Epoch 27/50\n",
      "\u001b[1m141/677\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1487 - loss: 5.1636"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    validation_data=(validation_inputs, validation_labels),\n",
    "    epochs=number_of_epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef216c6f-25c0-4d76-b152-531ec5a06fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.get_weights()[0]\n",
    "\n",
    "number_of_examples = 4\n",
    "for index in range(number_of_examples):\n",
    "    embedding = word_embeddings[index]\n",
    "    word = index2word[index]\n",
    "    pc(word, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c98a8-68b5-4125-822e-fb6eafc57d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 8))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692f466-2062-450e-aede-77f75429cdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
